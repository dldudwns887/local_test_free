{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "페이지 링크 싹다 가져오는 버전\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "def get_href_links(driver, wait, actions, num_items_to_fetch=100):\n",
    "    href_links = set()\n",
    "    while len(href_links) < num_items_to_fetch:\n",
    "        actions.send_keys(Keys.END)\n",
    "        time.sleep(2)\n",
    "        \n",
    "        for x in range(1, 46):\n",
    "            try:\n",
    "                xpath = f'//*[@id=\"root\"]/main/div/section[3]/div[1]/div/div[{x}]/div/div[2]/a[2]'\n",
    "                element = wait.until(EC.presence_of_element_located((By.XPATH, xpath)))\n",
    "                href = element.get_attribute('href')\n",
    "                href_links.add(href)\n",
    "            except (NoSuchElementException, TimeoutException):\n",
    "                continue\n",
    "\n",
    "        print(f\"Current number of unique href links: {len(href_links)}\")\n",
    "        if len(href_links) >= num_items_to_fetch:\n",
    "            break\n",
    "\n",
    "    return list(href_links)\n",
    "\n",
    "def extract_reviews(driver, wait):\n",
    "    actions = driver.find_element(By.CSS_SELECTOR, 'body')\n",
    "    time.sleep(1)\n",
    "    actions.send_keys(Keys.END)\n",
    "    time.sleep(2)\n",
    "    actions.send_keys(Keys.END)\n",
    "    time.sleep(2)\n",
    "    actions.send_keys(Keys.END)\n",
    "    time.sleep(1)\n",
    "\n",
    "    \n",
    "    reviews = []\n",
    "    for n in range(3, 10):\n",
    "        try:\n",
    "            review_id = wait.until(EC.presence_of_element_located((By.XPATH, f'//*[@id=\"style_estimate_list\"]/div/div[{n}]/div[1]/div/div[1]/p[1]'))).text\n",
    "            print(f\"review_id: {review_id}\")\n",
    "            #//*[@id=\"style_estimate_list\"]/div/div[3]/div[1]/div/div[1]/p[1]\n",
    "            try:\n",
    "                weight_height_gender = wait.until(EC.presence_of_element_located((By.XPATH, f'//*[@id=\"style_estimate_list\"]/div/div[{n}]/div[1]/div/div[2]/p[1]'))).text\n",
    "                print(f\"weight_height_gender: {weight_height_gender}\")\n",
    "            except:\n",
    "                weight_height_gender = wait.until(EC.presence_of_element_located((By.XPATH, f'//*[@id=\"style_estimate_list\"]/div/div[{n}]/div[1]/div/div[2]/p'))).text\n",
    "                print(f\"weight_height_gender (alternative): {weight_height_gender}\")\n",
    "            \n",
    "            \n",
    "            top_size = wait.until(EC.presence_of_element_located((By.XPATH, f'//*[@id=\"style_estimate_list\"]/div/div[{n}]/div[4]/div[1]/ul/li[1]/span'))).text\n",
    "            print(f\"top_size: {top_size}\")\n",
    "            \n",
    "            brightness_comment = wait.until(EC.presence_of_element_located((By.XPATH, f'//*[@id=\"style_estimate_list\"]/div/div[{n}]/div[4]/div[1]/ul/li[2]/span'))).text\n",
    "            print(f\"brightness_comment: {brightness_comment}\")\n",
    "            \n",
    "            color_comment = wait.until(EC.presence_of_element_located((By.XPATH, f'//*[@id=\"style_estimate_list\"]/div/div[{n}]/div[4]/div[1]/ul/li[3]/span'))).text\n",
    "            print(f\"color_comment: {color_comment}\")\n",
    "            \n",
    "            thickness_comment = wait.until(EC.presence_of_element_located((By.XPATH, f'//*[@id=\"style_estimate_list\"]/div/div[{n}]/div[4]/div[1]/ul/li[4]/span'))).text\n",
    "            print(f\"thickness_comment: {thickness_comment}\")\n",
    "            \n",
    "            purchased_product_id = wait.until(EC.presence_of_element_located((By.XPATH, f'//*[@id=\"style_estimate_list\"]/div/div[{n}]/div[2]/div[2]/a'))).get_attribute('href')\n",
    "            print(f\"purchased_product_id: {purchased_product_id}\")\n",
    "            \n",
    "            purchased_size = wait.until(EC.presence_of_element_located((By.XPATH, f'//*[@id=\"style_estimate_list\"]/div/div[{n}]/div[2]/div[2]/p/span'))).text\n",
    "            print(f\"purchased_size: {purchased_size}\")\n",
    "            \n",
    "            comment = wait.until(EC.presence_of_element_located((By.XPATH, f'//*[@id=\"style_estimate_list\"]/div/div[{n}]/div[4]/div[2]'))).text\n",
    "            print(f\"comment: {comment}\")\n",
    "\n",
    "            review = {\n",
    "                \"weight_height_gender\": weight_height_gender,\n",
    "                \"review_id\": review_id,\n",
    "                \"top_size\": top_size,\n",
    "                \"brightness_comment\": brightness_comment,\n",
    "                \"color_comment\": color_comment,\n",
    "                \"thickness_comment\": thickness_comment,\n",
    "                \"purchased_product_id\": purchased_product_id,\n",
    "                \"purchased_size\": purchased_size,\n",
    "                \"comment\": comment\n",
    "            }\n",
    "            reviews.append(review)\n",
    "        except (NoSuchElementException, TimeoutException):\n",
    "            print(f\"Review information not found for element index: {n}\")\n",
    "            continue\n",
    "    \n",
    "    return reviews\n",
    "\n",
    "def get_product_info(driver, wait, href_links):\n",
    "    products = []\n",
    "    for index, link in enumerate(href_links):\n",
    "        driver.get(link)\n",
    "        time.sleep(2)  # 페이지 로드 대기\n",
    "\n",
    "        try:\n",
    "            product_id = f\"top{index + 1}\"\n",
    "            \n",
    "            try:\n",
    "                product_name = wait.until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"root\"]/div[3]/h2'))).text\n",
    "            except (NoSuchElementException, TimeoutException):\n",
    "                product_name = \"N/A\"\n",
    "                print(f\"Product name not found for link: {link}\")\n",
    "                \n",
    "            try:\n",
    "                category = wait.until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"root\"]/div[2]/a[1]'))).text\n",
    "            except (NoSuchElementException, TimeoutException):\n",
    "                category = \"N/A\"\n",
    "                print(f\"Category not found for link: {link}\")\n",
    "\n",
    "            try:\n",
    "                price = wait.until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"root\"]/div[5]/div/div/span'))).text.strip()\n",
    "            except (NoSuchElementException, TimeoutException):\n",
    "                price = \"N/A\"\n",
    "                print(f\"Price not found for link: {link}\")\n",
    "\n",
    "            try:\n",
    "                image_element = wait.until(EC.presence_of_element_located((By.XPATH, '//img[@class=\"sc-1jl6n79-4 AqRjD\"]')))\n",
    "                image_url = image_element.get_attribute('src')\n",
    "            except (NoSuchElementException, TimeoutException):\n",
    "                image_url = \"N/A\"\n",
    "                print(f\"Image URL not found for link: {link}\")\n",
    "\n",
    "            description = link\n",
    "\n",
    "            # 사이즈 정보 가져오기\n",
    "            sizes = []\n",
    "            try:\n",
    "                ul_element = wait.until(EC.presence_of_element_located((By.XPATH, '//ul[contains(@class, \"sc-1sxlp32-1\") or contains(@class, \"sc-8wsa6t-1 Qtsoe\")]')))\n",
    "                li_elements = ul_element.find_elements(By.TAG_NAME, 'li')\n",
    "                for li in li_elements:\n",
    "                    size_text = li.text.strip()\n",
    "                    if size_text not in [\"cm\", \"MY\"]:\n",
    "                        sizes.append(size_text)\n",
    "            except (NoSuchElementException, TimeoutException):\n",
    "                print(f\"Size information not found for link: {link}\")\n",
    "\n",
    "            # 리뷰 정보 가져오기\n",
    "            reviews = extract_reviews(driver, wait)\n",
    "\n",
    "            product = {\n",
    "                \"product_id\": product_id,\n",
    "                \"product_name\": product_name,\n",
    "                \"category\": category,\n",
    "                \"price\": price,\n",
    "                \"image_url\": image_url,\n",
    "                \"description\": description,\n",
    "                \"size\": sizes,\n",
    "                \"reviews\": reviews\n",
    "            }\n",
    "            products.append(product)\n",
    "            \n",
    "            # 데이터 출력\n",
    "            print(product)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to extract data for link: {link} due to {e}\")\n",
    "            continue\n",
    "    \n",
    "    return products\n",
    "\n",
    "def save_to_csv(products, filename=\"products.csv\"):\n",
    "    df = pd.DataFrame(products)\n",
    "    df.to_csv(filename, index=False, encoding='utf-8-sig')\n",
    "    print(f\"Data saved to {filename}\")\n",
    "\n",
    "def main():\n",
    "    URL = \"https://www.musinsa.com/categories/item/001?device=mw&sortCode=emt_high\"\n",
    "    option = webdriver.ChromeOptions()\n",
    "    option.add_experimental_option(\"excludeSwitches\", [\"enable-logging\"])\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=option)\n",
    "    driver.get(URL)\n",
    "    time.sleep(3)\n",
    "\n",
    "    driver.find_element(By.XPATH, '//*[@id=\"root\"]/main/div/div[3]/div/button').click()\n",
    "    time.sleep(1)\n",
    "\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "    actions = driver.find_element(By.CSS_SELECTOR, 'body')\n",
    "\n",
    "    href_links = get_href_links(driver, wait, actions, num_items_to_fetch=100)\n",
    "    print(f\"Number of unique href links: {len(href_links)}\")\n",
    "\n",
    "    products = get_product_info(driver, wait, href_links)\n",
    "    print(\"Products extracted:\")\n",
    "    for product in products:\n",
    "        print(product)\n",
    "\n",
    "    save_to_csv(products)\n",
    "\n",
    "    # driver.quit()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 코드\n",
    "'''\n",
    "중간 부분이 안들어와져서 중간 부분부터 시작하는 코드인데 아마 너무 오래 걸려서 페이지 로딩이 느려져 안가져와지는 듯함\n",
    "중간 부분은 잘가져와진다 (xpath, class name 문제는 아님)\n",
    "\n",
    "'''\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "def get_href_links(driver, wait, actions, num_items_to_fetch=100):\n",
    "    href_links = set()\n",
    "    while len(href_links) < num_items_to_fetch:\n",
    "        actions.send_keys(Keys.END)\n",
    "        time.sleep(2)\n",
    "        \n",
    "        for x in range(1, 46):\n",
    "            try:\n",
    "                xpath = f'//*[@id=\"root\"]/main/div/section[3]/div[1]/div/div[{x}]/div/div[2]/a[2]'\n",
    "                element = wait.until(EC.presence_of_element_located((By.XPATH, xpath)))\n",
    "                href = element.get_attribute('href')\n",
    "                href_links.add(href)\n",
    "            except (NoSuchElementException, TimeoutException):\n",
    "                continue\n",
    "\n",
    "        print(f\"Current number of unique href links: {len(href_links)}\")\n",
    "        if len(href_links) >= num_items_to_fetch:\n",
    "            break\n",
    "\n",
    "    return list(href_links)\n",
    "\n",
    "def extract_reviews(driver, wait):\n",
    "    actions = driver.find_element(By.CSS_SELECTOR, 'body')\n",
    "    time.sleep(1)\n",
    "    actions.send_keys(Keys.END)\n",
    "    time.sleep(2)\n",
    "    actions.send_keys(Keys.END)\n",
    "    time.sleep(2)\n",
    "    actions.send_keys(Keys.END)\n",
    "    time.sleep(1)\n",
    "\n",
    "    \n",
    "    reviews = []\n",
    "    for n in range(3, 10):\n",
    "        try:\n",
    "            review_id = wait.until(EC.presence_of_element_located((By.XPATH, f'//*[@id=\"style_estimate_list\"]/div/div[{n}]/div[1]/div/div[1]/p[1]'))).text\n",
    "            print(f\"review_id: {review_id}\")\n",
    "            #//*[@id=\"style_estimate_list\"]/div/div[3]/div[1]/div/div[1]/p[1]\n",
    "            try:\n",
    "                weight_height_gender = wait.until(EC.presence_of_element_located((By.XPATH, f'//*[@id=\"style_estimate_list\"]/div/div[{n}]/div[1]/div/div[2]/p[1]'))).text\n",
    "                print(f\"weight_height_gender: {weight_height_gender}\")\n",
    "            except:\n",
    "                weight_height_gender = wait.until(EC.presence_of_element_located((By.XPATH, f'//*[@id=\"style_estimate_list\"]/div/div[{n}]/div[1]/div/div[2]/p'))).text\n",
    "                print(f\"weight_height_gender (alternative): {weight_height_gender}\")\n",
    "            \n",
    "            \n",
    "            top_size = wait.until(EC.presence_of_element_located((By.XPATH, f'//*[@id=\"style_estimate_list\"]/div/div[{n}]/div[4]/div[1]/ul/li[1]/span'))).text\n",
    "            print(f\"top_size: {top_size}\")\n",
    "            \n",
    "            brightness_comment = wait.until(EC.presence_of_element_located((By.XPATH, f'//*[@id=\"style_estimate_list\"]/div/div[{n}]/div[4]/div[1]/ul/li[2]/span'))).text\n",
    "            print(f\"brightness_comment: {brightness_comment}\")\n",
    "            \n",
    "            color_comment = wait.until(EC.presence_of_element_located((By.XPATH, f'//*[@id=\"style_estimate_list\"]/div/div[{n}]/div[4]/div[1]/ul/li[3]/span'))).text\n",
    "            print(f\"color_comment: {color_comment}\")\n",
    "            \n",
    "            thickness_comment = wait.until(EC.presence_of_element_located((By.XPATH, f'//*[@id=\"style_estimate_list\"]/div/div[{n}]/div[4]/div[1]/ul/li[4]/span'))).text\n",
    "            print(f\"thickness_comment: {thickness_comment}\")\n",
    "            \n",
    "            purchased_product_id = wait.until(EC.presence_of_element_located((By.XPATH, f'//*[@id=\"style_estimate_list\"]/div/div[{n}]/div[2]/div[2]/a'))).get_attribute('href')\n",
    "            print(f\"purchased_product_id: {purchased_product_id}\")\n",
    "            \n",
    "            purchased_size = wait.until(EC.presence_of_element_located((By.XPATH, f'//*[@id=\"style_estimate_list\"]/div/div[{n}]/div[2]/div[2]/p/span'))).text\n",
    "            print(f\"purchased_size: {purchased_size}\")\n",
    "            \n",
    "            comment = wait.until(EC.presence_of_element_located((By.XPATH, f'//*[@id=\"style_estimate_list\"]/div/div[{n}]/div[4]/div[2]'))).text\n",
    "            print(f\"comment: {comment}\")\n",
    "\n",
    "            review = {\n",
    "                \"weight_height_gender\": weight_height_gender,\n",
    "                \"review_id\": review_id,\n",
    "                \"top_size\": top_size,\n",
    "                \"brightness_comment\": brightness_comment,\n",
    "                \"color_comment\": color_comment,\n",
    "                \"thickness_comment\": thickness_comment,\n",
    "                \"purchased_product_id\": purchased_product_id,\n",
    "                \"purchased_size\": purchased_size,\n",
    "                \"comment\": comment\n",
    "            }\n",
    "            reviews.append(review)\n",
    "        except (NoSuchElementException, TimeoutException):\n",
    "            print(f\"Review information not found for element index: {n}\")\n",
    "            continue\n",
    "    \n",
    "    return reviews\n",
    "\n",
    "def get_product_info(driver, wait, href_links):\n",
    "    products = []\n",
    "    for index, link in enumerate(href_links[63:], start=64):  # 64번째 링크부터 시작\n",
    "        driver.get(link)\n",
    "        time.sleep(2)  # 페이지 로드 대기\n",
    "\n",
    "        try:\n",
    "            product_id = f\"top{index + 1}\"\n",
    "            \n",
    "            try:\n",
    "                product_name = wait.until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"root\"]/div[3]/h2'))).text\n",
    "            except (NoSuchElementException, TimeoutException):\n",
    "                product_name = \"N/A\"\n",
    "                print(f\"Product name not found for link: {link}\")\n",
    "                \n",
    "            try:\n",
    "                category = wait.until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"root\"]/div[2]/a[1]'))).text\n",
    "            except (NoSuchElementException, TimeoutException):\n",
    "                category = \"N/A\"\n",
    "                print(f\"Category not found for link: {link}\")\n",
    "\n",
    "            try:\n",
    "                price = wait.until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"root\"]/div[5]/div/div/span'))).text.strip()\n",
    "            except (NoSuchElementException, TimeoutException):\n",
    "                price = \"N/A\"\n",
    "                print(f\"Price not found for link: {link}\")\n",
    "\n",
    "            try:\n",
    "                image_element = wait.until(EC.presence_of_element_located((By.XPATH, '//img[@class=\"sc-1jl6n79-4 AqRjD\"]')))\n",
    "                image_url = image_element.get_attribute('src')\n",
    "            except (NoSuchElementException, TimeoutException):\n",
    "                image_url = \"N/A\"\n",
    "                print(f\"Image URL not found for link: {link}\")\n",
    "\n",
    "            description = link\n",
    "\n",
    "            # 사이즈 정보 가져오기\n",
    "            sizes = []\n",
    "            try:\n",
    "                ul_element = wait.until(EC.presence_of_element_located((By.XPATH, '//ul[contains(@class, \"sc-1sxlp32-1\") or contains(@class, \"sc-8wsa6t-1 Qtsoe\")]')))\n",
    "                li_elements = ul_element.find_elements(By.TAG_NAME, 'li')\n",
    "                for li in li_elements:\n",
    "                    size_text = li.text.strip()\n",
    "                    if size_text not in [\"cm\", \"MY\"]:\n",
    "                        sizes.append(size_text)\n",
    "            except (NoSuchElementException, TimeoutException):\n",
    "                print(f\"Size information not found for link: {link}\")\n",
    "\n",
    "            # 리뷰 정보 가져오기\n",
    "            reviews = extract_reviews(driver, wait)\n",
    "\n",
    "            product = {\n",
    "                \"product_id\": product_id,\n",
    "                \"product_name\": product_name,\n",
    "                \"category\": category,\n",
    "                \"price\": price,\n",
    "                \"image_url\": image_url,\n",
    "                \"description\": description,\n",
    "                \"size\": sizes,\n",
    "                \"reviews\": reviews\n",
    "            }\n",
    "            products.append(product)\n",
    "            \n",
    "            # 데이터 출력\n",
    "            print(product)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to extract data for link: {link} due to {e}\")\n",
    "            continue\n",
    "    \n",
    "    return products\n",
    "\n",
    "def save_to_csv(products, filename=\"product_test.csv\"):\n",
    "    df = pd.DataFrame(products)\n",
    "    df.to_csv(filename, index=False, encoding='utf-8-sig')\n",
    "    print(f\"Data saved to {filename}\")\n",
    "\n",
    "def main():\n",
    "    URL = \"https://www.musinsa.com/categories/item/001?device=mw&sortCode=emt_high\"\n",
    "    option = webdriver.ChromeOptions()\n",
    "    option.add_experimental_option(\"excludeSwitches\", [\"enable-logging\"])\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=option)\n",
    "    driver.get(URL)\n",
    "    time.sleep(3)\n",
    "\n",
    "    driver.find_element(By.XPATH, '//*[@id=\"root\"]/main/div/div[3]/div/button').click()\n",
    "    time.sleep(1)\n",
    "\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "    actions = driver.find_element(By.CSS_SELECTOR, 'body')\n",
    "\n",
    "    href_links = get_href_links(driver, wait, actions, num_items_to_fetch=100)\n",
    "    print(f\"Number of unique href links: {len(href_links)}\")\n",
    "\n",
    "    products = get_product_info(driver, wait, href_links)\n",
    "    print(\"Products extracted:\")\n",
    "    for product in products:\n",
    "        print(product)\n",
    "\n",
    "    save_to_csv(products, filename=\"product_test.csv\")\n",
    "\n",
    "    # driver.quit()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 멀티쓰레드 테스트\n",
    "'''\n",
    "쓰레드 테스트 결과 90분에서 40분으로 단축\n",
    "이제 남은 것은 color 정보를 가져오기만 하면됨.\n",
    "\n",
    "color 정보는 고려해봐야할 듯 함.\n",
    "\n",
    "'''\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import pandas as pd\n",
    "import concurrent.futures\n",
    "\n",
    "def get_href_links(driver, wait, actions, num_items_to_fetch=100):\n",
    "    href_links = set()\n",
    "    while len(href_links) < num_items_to_fetch:\n",
    "        actions.send_keys(Keys.END)\n",
    "        time.sleep(2)\n",
    "        \n",
    "        for x in range(1, 46):\n",
    "            try:\n",
    "                xpath = f'//*[@id=\"root\"]/main/div/section[3]/div[1]/div/div[{x}]/div/div[2]/a[2]'\n",
    "                element = wait.until(EC.presence_of_element_located((By.XPATH, xpath)))\n",
    "                href = element.get_attribute('href')\n",
    "                href_links.add(href)\n",
    "            except (NoSuchElementException, TimeoutException):\n",
    "                continue\n",
    "\n",
    "        print(f\"Current number of unique href links: {len(href_links)}\")\n",
    "        if len(href_links) >= num_items_to_fetch:\n",
    "            break\n",
    "\n",
    "    return list(href_links)\n",
    "\n",
    "def extract_reviews(driver, wait):\n",
    "    actions = driver.find_element(By.CSS_SELECTOR, 'body')\n",
    "    time.sleep(1)\n",
    "    actions.send_keys(Keys.END)\n",
    "    time.sleep(2)\n",
    "    actions.send_keys(Keys.END)\n",
    "    time.sleep(2)\n",
    "    actions.send_keys(Keys.END)\n",
    "    time.sleep(1)\n",
    "\n",
    "    reviews = []\n",
    "    for n in range(3, 10):\n",
    "        try:\n",
    "            review_id = wait.until(EC.presence_of_element_located((By.XPATH, f'//*[@id=\"style_estimate_list\"]/div/div[{n}]/div[1]/div/div[1]/p[1]'))).text\n",
    "            print(f\"review_id: {review_id}\")\n",
    "            \n",
    "            try:\n",
    "                weight_height_gender = wait.until(EC.presence_of_element_located((By.XPATH, f'//*[@id=\"style_estimate_list\"]/div/div[{n}]/div[1]/div/div[2]/p[1]'))).text\n",
    "                print(f\"weight_height_gender: {weight_height_gender}\")\n",
    "            except:\n",
    "                weight_height_gender = wait.until(EC.presence_of_element_located((By.XPATH, f'//*[@id=\"style_estimate_list\"]/div/div[{n}]/div[1]/div/div[2]/p'))).text\n",
    "                print(f\"weight_height_gender (alternative): {weight_height_gender}\")\n",
    "            \n",
    "            top_size = wait.until(EC.presence_of_element_located((By.XPATH, f'//*[@id=\"style_estimate_list\"]/div/div[{n}]/div[4]/div[1]/ul/li[1]/span'))).text\n",
    "            print(f\"top_size: {top_size}\")\n",
    "            \n",
    "            brightness_comment = wait.until(EC.presence_of_element_located((By.XPATH, f'//*[@id=\"style_estimate_list\"]/div/div[{n}]/div[4]/div[1]/ul/li[2]/span'))).text\n",
    "            print(f\"brightness_comment: {brightness_comment}\")\n",
    "            \n",
    "            color_comment = wait.until(EC.presence_of_element_located((By.XPATH, f'//*[@id=\"style_estimate_list\"]/div/div[{n}]/div[4]/div[1]/ul/li[3]/span'))).text\n",
    "            print(f\"color_comment: {color_comment}\")\n",
    "            \n",
    "            thickness_comment = wait.until(EC.presence_of_element_located((By.XPATH, f'//*[@id=\"style_estimate_list\"]/div/div[{n}]/div[4]/div[1]/ul/li[4]/span'))).text\n",
    "            print(f\"thickness_comment: {thickness_comment}\")\n",
    "            \n",
    "            purchased_product_id = wait.until(EC.presence_of_element_located((By.XPATH, f'//*[@id=\"style_estimate_list\"]/div/div[{n}]/div[2]/div[2]/a'))).get_attribute('href')\n",
    "            print(f\"purchased_product_id: {purchased_product_id}\")\n",
    "            \n",
    "            purchased_size = wait.until(EC.presence_of_element_located((By.XPATH, f'//*[@id=\"style_estimate_list\"]/div/div[{n}]/div[2]/div[2]/p/span'))).text\n",
    "            print(f\"purchased_size: {purchased_size}\")\n",
    "            \n",
    "            comment = wait.until(EC.presence_of_element_located((By.XPATH, f'//*[@id=\"style_estimate_list\"]/div/div[{n}]/div[4]/div[2]'))).text\n",
    "            print(f\"comment: {comment}\")\n",
    "\n",
    "            review = {\n",
    "                \"weight_height_gender\": weight_height_gender,\n",
    "                \"review_id\": review_id,\n",
    "                \"top_size\": top_size,\n",
    "                \"brightness_comment\": brightness_comment,\n",
    "                \"color_comment\": color_comment,\n",
    "                \"thickness_comment\": thickness_comment,\n",
    "                \"purchased_product_id\": purchased_product_id,\n",
    "                \"purchased_size\": purchased_size,\n",
    "                \"comment\": comment\n",
    "            }\n",
    "            reviews.append(review)\n",
    "        except (NoSuchElementException, TimeoutException):\n",
    "            print(f\"Review information not found for element index: {n}\")\n",
    "            continue\n",
    "    \n",
    "    return reviews\n",
    "\n",
    "def get_product_info(driver, wait, href_links):\n",
    "    products = []\n",
    "    for index, link in enumerate(href_links):\n",
    "        driver.get(link)\n",
    "        time.sleep(2)  # 페이지 로드 대기\n",
    "\n",
    "        try:\n",
    "            product_id = f\"top{index + 1}\"\n",
    "            \n",
    "            try:\n",
    "                product_name = wait.until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"root\"]/div[3]/h2'))).text\n",
    "            except (NoSuchElementException, TimeoutException):\n",
    "                product_name = \"N/A\"\n",
    "                print(f\"Product name not found for link: {link}\")\n",
    "                \n",
    "            try:\n",
    "                category = wait.until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"root\"]/div[2]/a[1]'))).text\n",
    "            except (NoSuchElementException, TimeoutException):\n",
    "                category = \"N/A\"\n",
    "                print(f\"Category not found for link: {link}\")\n",
    "\n",
    "            try:\n",
    "                price = wait.until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"root\"]/div[5]/div/div/span'))).text.strip()\n",
    "            except (NoSuchElementException, TimeoutException):\n",
    "                price = \"N/A\"\n",
    "                print(f\"Price not found for link: {link}\")\n",
    "\n",
    "            try:\n",
    "                image_element = wait.until(EC.presence_of_element_located((By.XPATH, '//img[@class=\"sc-1jl6n79-4 AqRjD\"]')))\n",
    "                image_url = image_element.get_attribute('src')\n",
    "            except (NoSuchElementException, TimeoutException):\n",
    "                image_url = \"N/A\"\n",
    "                print(f\"Image URL not found for link: {link}\")\n",
    "\n",
    "            description = link\n",
    "\n",
    "            sizes = []\n",
    "            try:\n",
    "                ul_element = wait.until(EC.presence_of_element_located((By.XPATH, '//ul[contains(@class, \"sc-1sxlp32-1\") or contains(@class, \"sc-8wsa6t-1 Qtsoe\")]')))\n",
    "                li_elements = ul_element.find_elements(By.TAG_NAME, 'li')\n",
    "                for li in li_elements:\n",
    "                    size_text = li.text.strip()\n",
    "                    if size_text not in [\"cm\", \"MY\"]:\n",
    "                        sizes.append(size_text)\n",
    "            except (NoSuchElementException, TimeoutException):\n",
    "                print(f\"Size information not found for link: {link}\")\n",
    "\n",
    "            reviews = extract_reviews(driver, wait)\n",
    "\n",
    "            product = {\n",
    "                \"product_id\": product_id,\n",
    "                \"product_name\": product_name,\n",
    "                \"category\": category,\n",
    "                \"price\": price,\n",
    "                \"image_url\": image_url,\n",
    "                \"description\": description,\n",
    "                \"size\": sizes,\n",
    "                \"reviews\": reviews\n",
    "            }\n",
    "            products.append(product)\n",
    "            \n",
    "            print(product)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to extract data for link: {link} due to {e}\")\n",
    "            continue\n",
    "    \n",
    "    return products\n",
    "\n",
    "def save_to_csv(products, filename=\"products.csv\"):\n",
    "    df = pd.DataFrame(products)\n",
    "    df.to_csv(filename, index=False, encoding='utf-8-sig')\n",
    "    print(f\"Data saved to {filename}\")\n",
    "\n",
    "def split_list(lst, n):\n",
    "    k, m = divmod(len(lst), n)\n",
    "    return [lst[i * k + min(i, m):(i + 1) * k + min(i + 1, m)] for i in range(n)]\n",
    "\n",
    "def process_links(links):\n",
    "    option = webdriver.ChromeOptions()\n",
    "    option.add_experimental_option(\"excludeSwitches\", [\"enable-logging\"])\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=option)\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "\n",
    "    products = get_product_info(driver, wait, links)\n",
    "    driver.quit()\n",
    "    return products\n",
    "\n",
    "def main():\n",
    "    URL = \"https://www.musinsa.com/categories/item/001?device=mw&sortCode=emt_high\"\n",
    "    option = webdriver.ChromeOptions()\n",
    "    option.add_experimental_option(\"excludeSwitches\", [\"enable-logging\"])\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=option)\n",
    "    driver.get(URL)\n",
    "    time.sleep(3)\n",
    "\n",
    "    driver.find_element(By.XPATH, '//*[@id=\"root\"]/main/div/div[3]/div/button').click()\n",
    "    time.sleep(1)\n",
    "\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "    actions = driver.find_element(By.CSS_SELECTOR, 'body')\n",
    "\n",
    "    href_links = get_href_links(driver, wait, actions, num_items_to_fetch=100)\n",
    "    driver.quit()\n",
    "\n",
    "    sub_lists = split_list(href_links, 4)\n",
    "    # Thread info\n",
    "    # https://velog.io/@cha-suyeon/%ED%8C%8C%EC%9D%B4%EC%8D%AC%EC%97%90%EC%84%9C-%EC%8A%A4%EB%A0%88%EB%93%9C%ED%94%84%EB%A1%9C%EC%84%B8%EC%8A%A4-%ED%92%80-%EC%82%AC%EC%9A%A9%ED%95%98%EA%B8%B0\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        future_to_links = {executor.submit(process_links, sub_list): sub_list for sub_list in sub_lists}\n",
    "        all_products = []\n",
    "        for future in concurrent.futures.as_completed(future_to_links):\n",
    "            products = future.result()\n",
    "            all_products.extend(products)\n",
    "\n",
    "    save_to_csv(all_products)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thread count share update\n",
    "'''\n",
    "쓰레드 테스트 결과 90분에서 40분으로 단축\n",
    "이제 남은 것은 color 정보를 가져오기만 하면됨.\n",
    "\n",
    "color 정보는 고려해봐야할 듯 함.\n",
    "\n",
    "+\n",
    "\n",
    "csv파일 조회 결과 쓰레드 간 카운트를 공유하지 않는 문제 발생.\n",
    "이를 해결하기 위해 수정.\n",
    "\n",
    "\n",
    "'''\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import pandas as pd\n",
    "import concurrent.futures\n",
    "from multiprocessing import Manager\n",
    "\n",
    "def get_href_links(driver, wait, actions, num_items_to_fetch=100):\n",
    "    href_links = set()\n",
    "    while len(href_links) < num_items_to_fetch:\n",
    "        actions.send_keys(Keys.END)\n",
    "        time.sleep(2)\n",
    "        \n",
    "        for x in range(1, 46):\n",
    "            try:\n",
    "                xpath = f'//*[@id=\"root\"]/main/div/section[3]/div[1]/div/div[{x}]/div/div[2]/a[2]'\n",
    "                element = wait.until(EC.presence_of_element_located((By.XPATH, xpath)))\n",
    "                href = element.get_attribute('href')\n",
    "                href_links.add(href)\n",
    "            except (NoSuchElementException, TimeoutException):\n",
    "                continue\n",
    "\n",
    "        print(f\"Current number of unique href links: {len(href_links)}\")\n",
    "        if len(href_links) >= num_items_to_fetch:\n",
    "            break\n",
    "\n",
    "    return list(href_links)\n",
    "\n",
    "def extract_reviews(driver, wait):\n",
    "    actions = driver.find_element(By.CSS_SELECTOR, 'body')\n",
    "    time.sleep(1)\n",
    "    actions.send_keys(Keys.END)\n",
    "    time.sleep(2)\n",
    "    actions.send_keys(Keys.END)\n",
    "    time.sleep(2)\n",
    "    actions.send_keys(Keys.END)\n",
    "    time.sleep(1)\n",
    "\n",
    "    reviews = []\n",
    "    for n in range(3, 10):\n",
    "        try:\n",
    "            review_id = wait.until(EC.presence_of_element_located((By.XPATH, f'//*[@id=\"style_estimate_list\"]/div/div[{n}]/div[1]/div/div[1]/p[1]'))).text\n",
    "            print(f\"review_id: {review_id}\")\n",
    "            \n",
    "            try:\n",
    "                weight_height_gender = wait.until(EC.presence_of_element_located((By.XPATH, f'//*[@id=\"style_estimate_list\"]/div/div[{n}]/div[1]/div/div[2]/p[1]'))).text\n",
    "                print(f\"weight_height_gender: {weight_height_gender}\")\n",
    "            except:\n",
    "                weight_height_gender = wait.until(EC.presence_of_element_located((By.XPATH, f'//*[@id=\"style_estimate_list\"]/div/div[{n}]/div[1]/div/div[2]/p'))).text\n",
    "                print(f\"weight_height_gender (alternative): {weight_height_gender}\")\n",
    "            \n",
    "            top_size = wait.until(EC.presence_of_element_located((By.XPATH, f'//*[@id=\"style_estimate_list\"]/div/div[{n}]/div[4]/div[1]/ul/li[1]/span'))).text\n",
    "            print(f\"top_size: {top_size}\")\n",
    "            \n",
    "            brightness_comment = wait.until(EC.presence_of_element_located((By.XPATH, f'//*[@id=\"style_estimate_list\"]/div/div[{n}]/div[4]/div[1]/ul/li[2]/span'))).text\n",
    "            print(f\"brightness_comment: {brightness_comment}\")\n",
    "            \n",
    "            color_comment = wait.until(EC.presence_of_element_located((By.XPATH, f'//*[@id=\"style_estimate_list\"]/div/div[{n}]/div[4]/div[1]/ul/li[3]/span'))).text\n",
    "            print(f\"color_comment: {color_comment}\")\n",
    "            \n",
    "            thickness_comment = wait.until(EC.presence_of_element_located((By.XPATH, f'//*[@id=\"style_estimate_list\"]/div/div[{n}]/div[4]/div[1]/ul/li[4]/span'))).text\n",
    "            print(f\"thickness_comment: {thickness_comment}\")\n",
    "            \n",
    "            purchased_product_id = wait.until(EC.presence_of_element_located((By.XPATH, f'//*[@id=\"style_estimate_list\"]/div/div[{n}]/div[2]/div[2]/a'))).get_attribute('href')\n",
    "            print(f\"purchased_product_id: {purchased_product_id}\")\n",
    "            \n",
    "            purchased_size = wait.until(EC.presence_of_element_located((By.XPATH, f'//*[@id=\"style_estimate_list\"]/div/div[{n}]/div[2]/div[2]/p/span'))).text\n",
    "            print(f\"purchased_size: {purchased_size}\")\n",
    "            \n",
    "            comment = wait.until(EC.presence_of_element_located((By.XPATH, f'//*[@id=\"style_estimate_list\"]/div/div[{n}]/div[4]/div[2]'))).text\n",
    "            print(f\"comment: {comment}\")\n",
    "\n",
    "            review = {\n",
    "                \"weight_height_gender\": weight_height_gender,\n",
    "                \"review_id\": review_id,\n",
    "                \"top_size\": top_size,\n",
    "                \"brightness_comment\": brightness_comment,\n",
    "                \"color_comment\": color_comment,\n",
    "                \"thickness_comment\": thickness_comment,\n",
    "                \"purchased_product_id\": purchased_product_id,\n",
    "                \"purchased_size\": purchased_size,\n",
    "                \"comment\": comment\n",
    "            }\n",
    "            reviews.append(review)\n",
    "        except (NoSuchElementException, TimeoutException):\n",
    "            print(f\"Review information not found for element index: {n}\")\n",
    "            continue\n",
    "    \n",
    "    return reviews\n",
    "\n",
    "def get_product_info(driver, wait, href_links, count):\n",
    "    products = []\n",
    "    for index, link in enumerate(href_links):\n",
    "        driver.get(link)\n",
    "        time.sleep(2)\n",
    "\n",
    "        try:\n",
    "            product_id = f\"top{index + 1}\"\n",
    "            \n",
    "            try:\n",
    "                product_name = wait.until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"root\"]/div[3]/h2'))).text\n",
    "            except (NoSuchElementException, TimeoutException):\n",
    "                product_name = \"N/A\"\n",
    "                print(f\"Product name not found for link: {link}\")\n",
    "                \n",
    "            try:\n",
    "                category = wait.until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"root\"]/div[2]/a[1]'))).text\n",
    "            except (NoSuchElementException, TimeoutException):\n",
    "                category = \"N/A\"\n",
    "                print(f\"Category not found for link: {link}\")\n",
    "\n",
    "            try:\n",
    "                price = wait.until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"root\"]/div[5]/div/div/span'))).text.strip()\n",
    "            except (NoSuchElementException, TimeoutException):\n",
    "                price = \"N/A\"\n",
    "                print(f\"Price not found for link: {link}\")\n",
    "\n",
    "            try:\n",
    "                image_element = wait.until(EC.presence_of_element_located((By.XPATH, '//img[@class=\"sc-1jl6n79-4 AqRjD\"]')))\n",
    "                image_url = image_element.get_attribute('src')\n",
    "            except (NoSuchElementException, TimeoutException):\n",
    "                image_url = \"N/A\"\n",
    "                print(f\"Image URL not found for link: {link}\")\n",
    "\n",
    "            description = link\n",
    "\n",
    "            sizes = []\n",
    "            try:\n",
    "                ul_element = wait.until(EC.presence_of_element_located((By.XPATH, '//ul[contains(@class, \"sc-1sxlp32-1\") or contains(@class, \"sc-8wsa6t-1 Qtsoe\")]')))\n",
    "                li_elements = ul_element.find_elements(By.TAG_NAME, 'li')\n",
    "                for li in li_elements:\n",
    "                    size_text = li.text.strip()\n",
    "                    if size_text not in [\"cm\", \"MY\"]:\n",
    "                        sizes.append(size_text)\n",
    "            except (NoSuchElementException, TimeoutException):\n",
    "                print(f\"Size information not found for link: {link}\")\n",
    "\n",
    "            reviews = extract_reviews(driver, wait)\n",
    "\n",
    "            product = {\n",
    "                \"product_id\": product_id,\n",
    "                \"product_name\": product_name,\n",
    "                \"category\": category,\n",
    "                \"price\": price,\n",
    "                \"image_url\": image_url,\n",
    "                \"description\": description,\n",
    "                \"size\": sizes,\n",
    "                \"reviews\": reviews\n",
    "            }\n",
    "            products.append(product)\n",
    "            \n",
    "            print(product)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to extract data for link: {link} due to {e}\")\n",
    "            continue\n",
    "\n",
    "        with count.get_lock():\n",
    "            count.value += 1\n",
    "            print(f\"Processed {count.value} products\")\n",
    "\n",
    "    return products\n",
    "\n",
    "def save_to_csv(products, filename=\"products.csv\"):\n",
    "    df = pd.DataFrame(products)\n",
    "    df.to_csv(filename, index=False, encoding='utf-8-sig')\n",
    "    print(f\"Data saved to {filename}\")\n",
    "\n",
    "def split_list(lst, n):\n",
    "    k, m = divmod(len(lst), n)\n",
    "    return [lst[i * k + min(i, m):(i + 1) * k + min(i + 1, m)] for i in range(n)]\n",
    "\n",
    "def process_links(links, count):\n",
    "    option = webdriver.ChromeOptions()\n",
    "    option.add_experimental_option(\"excludeSwitches\", [\"enable-logging\"])\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=option)\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "\n",
    "    products = get_product_info(driver, wait, links, count)\n",
    "    driver.quit()\n",
    "    return products\n",
    "\n",
    "def main():\n",
    "    URL = \"https://www.musinsa.com/categories/item/001?device=mw&sortCode=emt_high\"\n",
    "    option = webdriver.ChromeOptions()\n",
    "    option.add_experimental_option(\"excludeSwitches\", [\"enable-logging\"])\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=option)\n",
    "    driver.get(URL)\n",
    "    time.sleep(3)\n",
    "\n",
    "    driver.find_element(By.XPATH, '//*[@id=\"root\"]/main/div/div[3]/div/button').click()\n",
    "    time.sleep(1)\n",
    "\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "    actions = driver.find_element(By.CSS_SELECTOR, 'body')\n",
    "\n",
    "    href_links = get_href_links(driver, wait, actions, num_items_to_fetch=100)\n",
    "    driver.quit()\n",
    "\n",
    "    sub_lists = split_list(href_links, 4)\n",
    "\n",
    "    manager = Manager()\n",
    "    count = manager.Value('i', 0)\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        futures = [executor.submit(process_links, sub_list, count) for sub_list in sub_lists]\n",
    "        all_products = []\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            products = future.result()\n",
    "            all_products.extend(products)\n",
    "\n",
    "    save_to_csv(all_products)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "현재까지 color 정보를 제외한 모든 정보를 가져올 수 있음\n",
    "\n",
    "그러면 남은 과제는 무엇일까?\n",
    "\n",
    "color 정보를 가져와야 함.\n",
    "\n",
    "기존에 csv에서 description에서 이미 존재한 상품 링크이면 조회하지 않고 새로운 것만 \"추가\"하는 방식으로 가야함. (업데이트 시 시간 단축)\n",
    "\n",
    "나중에 s3연동이 되면 s3로 업로드를 하는 방식으로 가야함 (링크 조회는 아마 데이터 웨어하우스에서하거나 s3를 쿼리처럼 하는 방식처럼 이용)\n",
    "\n",
    "주기적으로 실행하기 위해 airflow dag code로 transform해야함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# CSV 파일을 읽어 데이터 프레임으로 변환\n",
    "df = pd.read_csv('products_multi_Thread.csv', encoding='utf-8-sig')\n",
    "\n",
    "# description 열만 추출하여 리스트로 변환\n",
    "description_list = df['description'].tolist()\n",
    "\n",
    "# 추출된 description 리스트 출력\n",
    "print(description_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이 코드는 색깔과 사이즈를 구하는 코드\n",
    "\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException, ElementClickInterceptedException\n",
    "import time\n",
    "\n",
    "# CSV 파일을 읽어 데이터 프레임으로 변환\n",
    "df = pd.read_csv('products_multi_Thread.csv', encoding='utf-8-sig')\n",
    "\n",
    "# description 열만 추출하여 리스트로 변환\n",
    "description_list = df['description'].tolist()\n",
    "#테스트 끝나면 이부분삭제###################################\n",
    "#description_list = description_list[:5]\n",
    "\n",
    "def get_li_texts(driver, xpath):\n",
    "    ul_element = driver.find_element(By.XPATH, xpath)\n",
    "    li_elements = ul_element.find_elements(By.TAG_NAME, 'li')\n",
    "    return [li.text for li in li_elements]\n",
    "\n",
    "def visit_websites(description_list):\n",
    "    option = webdriver.ChromeOptions()\n",
    "    option.add_experimental_option(\"excludeSwitches\", [\"enable-logging\"])\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=option)\n",
    "\n",
    "    for url in description_list:\n",
    "        try:\n",
    "            driver.get(url)\n",
    "            time.sleep(2)  # 페이지 로드 대기\n",
    "            print(f\"Visited: {url}\")\n",
    "            \n",
    "            button_clicked = False\n",
    "            for n in range(31, 42):\n",
    "                try:\n",
    "                    xpath = f'//*[@id=\"root\"]/div[{n}]/div/button'\n",
    "                    button = driver.find_element(By.XPATH, xpath)\n",
    "                    driver.execute_script(\"arguments[0].scrollIntoView(true);\", button)\n",
    "                    time.sleep(1)  # 스크롤 후 대기\n",
    "                    button.click()\n",
    "                    print(f\"Clicked button at div[{n}] on {url}\")\n",
    "                    button_clicked = True\n",
    "\n",
    "                    # 다음 단계 수행\n",
    "                    next_n = n + 2\n",
    "                    try:\n",
    "                        \n",
    "                        button_1_xpath = f'//*[@id=\"root\"]/div[{next_n}]/div[1]/div[1]/div[1]/div[1]/button[1]'\n",
    "                        button_2_xpath = f'//*[@id=\"root\"]/div[{next_n}]/div[1]/div[1]/div[1]/div[1]/button[2]'\n",
    "                        \n",
    "                        driver.find_element(By.XPATH, button_1_xpath).click()\n",
    "                        print(f\"Clicked button 1 at div[{next_n}] on {url}\")\n",
    "                        test_option_1 = get_li_texts(driver, f'//*[@id=\"root\"]/div[{next_n}]/div[2]/ul')\n",
    "                        \n",
    "                        try:\n",
    "                            driver.find_element(By.XPATH, f'//*[@id=\"root\"]/div[{next_n}]/div[2]/ul/li[1]').click()\n",
    "                        except NoSuchElementException:\n",
    "                            pass\n",
    "\n",
    "                        driver.find_element(By.XPATH, button_2_xpath).click()\n",
    "                        print(f\"Clicked button 2 at div[{next_n}] on {url}\")\n",
    "                        test_option_2 = get_li_texts(driver, f'//*[@id=\"root\"]/div[{next_n}]/div[2]/ul')\n",
    "                    \n",
    "                    except NoSuchElementException:\n",
    "                        try:\n",
    "                            button_xpath = f'//*[@id=\"root\"]/div[{next_n}]/div[1]/div[1]/div[1]/div[1]/button'\n",
    "                            driver.find_element(By.XPATH, button_xpath).click()\n",
    "                            print(f\"Clicked button at div[{next_n}] on {url}\")\n",
    "                            test_option_1 = get_li_texts(driver, f'//*[@id=\"root\"]/div[{next_n}]/div[2]/ul')\n",
    "                            test_option_2 = ['0']\n",
    "                        except NoSuchElementException:\n",
    "                            print(f\"No additional buttons found at div[{next_n}] on {url}\")\n",
    "                            test_option_1 = []\n",
    "                            test_option_2 = []\n",
    "\n",
    "                    print(f\"test_option_1 for {url}: {test_option_1}\")\n",
    "                    print(f\"test_option_2 for {url}: {test_option_2}\")\n",
    "                    break  # 버튼을 클릭했으면 루프를 벗어납니다\n",
    "\n",
    "                except (NoSuchElementException, TimeoutException):\n",
    "                    continue  # 현재 div[{n}]에서 버튼을 찾지 못했으면 다음으로 넘어갑니다\n",
    "                except ElementClickInterceptedException:\n",
    "                    print(f\"Element click intercepted on {url} at div[{n}], trying next element.\")\n",
    "                    continue  # 클릭이 차단된 경우 다음으로 넘어갑니다\n",
    "\n",
    "            if not button_clicked:\n",
    "                print(f\"No button found on {url}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to visit {url} due to {e}\")\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "# 각 description URL을 순회하며 방문\n",
    "visit_websites(description_list)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
